{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "import imblearn\n",
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score , recall_score\n",
    "\n",
    "from fairlearn.metrics import MetricFrame, false_positive_rate, false_negative_rate,count, selection_rate\n",
    "from fairlearn.datasets import fetch_adult\n",
    "from fairlearn.postprocessing import ThresholdOptimizer, plot_threshold_optimizer\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_ratio\n",
    "from fairlearn.reductions import DemographicParity\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotes and replace spaces with underscores\n",
    "train_data.columns = [col.strip().replace(\"'\", '').replace(' ', '_') for col in train_data.columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.rename(columns={\n",
    "    'Genes_in_mothers_side': 'Genes_Mothers_Side',\n",
    "    'Inherited_from_father': 'Inherited_Father',\n",
    "    'Blood_cell_count_(mcL)': 'Blood_Cell_mcL',\n",
    "    'Respiratory_Rate_(breaths/min)': 'Respiratory_Rate_breaths_min',\n",
    "    'Heart_Rate_(rates/min': 'Heart_Rates_Min',\n",
    "    'Follow-up': 'Follow_up',\n",
    "    'Autopsy_shows_birth_defect_(if_applicable)': 'Autopsy_Birth_Defect',\n",
    "    'Folic_acid_details_(peri-conceptional)': 'Folic_Acid',\n",
    "    'H/O_serious_maternal_illness': 'Maternal_Illness',\n",
    "    'H/O_radiation_exposure_(x-ray)': 'Radiation_Exposure',\n",
    "    'H/O_substance_abuse': 'Substance_Abuse',\n",
    "    'Assisted_conception_IVF/ART': 'Assisted_Conception',\n",
    "    'History_of_anomalies_in_previous_pregnancies': 'History_Previous_Pregnancies',\n",
    "    'No._of_previous_abortion': 'Previous_Abortion',\n",
    "    'Birth_defects': 'Birth_Defects',\n",
    "    'White_Blood_cell_count_(thousand_per_microliter)': 'White_Blood_Cell',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'Genetic_Disorder' is NaN\n",
    "train_data.dropna(subset=['Genetic_Disorder'], inplace=True)\n",
    "\n",
    "# Reset the index to update row numbers and avoid keeping the old index\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Chi-Square Test for Each Feature\n",
    "    - to evaluate feature improtance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Assuming train_data is already loaded\n",
    "# Define X (all categorical features except the target) and y (target variable)\n",
    "X = train_data.drop(columns=['Genetic_Disorder', 'Disorder_Subclass'])  # Features\n",
    "y = train_data['Genetic_Disorder']  # Target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Feature     Chi2 Stat        p-value\n",
      "0                     Patient_Id  39874.000000   4.962328e-01\n",
      "6                 Blood_Cell_mcL  39874.000000   4.962328e-01\n",
      "36              White_Blood_Cell  30969.304917   9.410793e-01\n",
      "9                   Fathers_name  30507.493656   2.989873e-01\n",
      "8                    Family_Name  12018.213711   3.519372e-01\n",
      "7             Patient_First_Name   4988.118109   9.422057e-02\n",
      "42                     Symptom_5   1001.441227  3.465776e-218\n",
      "41                     Symptom_4    653.909808  1.012253e-142\n",
      "40                     Symptom_3    587.264659  2.999850e-128\n",
      "39                     Symptom_2    386.997596   9.215941e-85\n",
      "38                     Symptom_1    210.217763   2.247982e-46\n",
      "2             Genes_Mothers_Side    174.722576   1.146766e-38\n",
      "3               Inherited_Father    172.798123   3.001680e-38\n",
      "4                  Maternal_gene    114.615651   1.292789e-25\n",
      "5                  Paternal_gene    112.651041   3.452529e-25\n",
      "11                   Fathers_age     92.794980   3.426921e-01\n",
      "12                Institute_Name     53.619773   4.119717e-01\n",
      "10                   Mothers_age     46.917226   9.637048e-01\n",
      "13         Location_of_Institute     46.227686   6.255166e-01\n",
      "37             Blood_test_result     21.213682   1.679258e-03\n",
      "1                    Patient_Age     16.778439   9.528380e-01\n",
      "34             Previous_Abortion      9.196632   3.259810e-01\n",
      "24                        Gender      6.014716   1.980519e-01\n",
      "25                Birth_asphyxia      4.802947   5.693238e-01\n",
      "35                 Birth_Defects      3.116307   2.105245e-01\n",
      "31               Substance_Abuse      2.975520   8.119131e-01\n",
      "33  History_Previous_Pregnancies      2.685539   2.611214e-01\n",
      "27                Place_of_birth      2.483794   2.888358e-01\n",
      "30            Radiation_Exposure      1.635852   9.499653e-01\n",
      "28                    Folic_Acid      1.478145   4.775566e-01\n",
      "16               Heart_Rates_Min      1.411389   4.937655e-01\n",
      "23                     Follow_up      1.387392   4.997256e-01\n",
      "15  Respiratory_Rate_breaths_min      1.263187   5.317438e-01\n",
      "26          Autopsy_Birth_Defect      1.134742   8.887216e-01\n",
      "32           Assisted_Conception      0.715736   6.991655e-01\n",
      "14                        Status      0.691741   7.076041e-01\n",
      "29              Maternal_Illness      0.373224   8.297656e-01\n",
      "22              Parental_consent      0.000000   1.000000e+00\n",
      "20                        Test_4      0.000000   1.000000e+00\n",
      "19                        Test_3      0.000000   1.000000e+00\n",
      "18                        Test_2      0.000000   1.000000e+00\n",
      "17                        Test_1      0.000000   1.000000e+00\n",
      "21                        Test_5      0.000000   1.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store results\n",
    "chi2_results = []\n",
    "\n",
    "# Loop through each feature in X\n",
    "for column in X.columns:\n",
    "    # Create a contingency table for the feature and the target\n",
    "    contingency_table = pd.crosstab(X[column], y)\n",
    "    \n",
    "    # Perform the Chi-Square test\n",
    "    chi2_stat, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    # Append results (feature name, Chi-Square statistic, p-value)\n",
    "    chi2_results.append({'Feature': column, 'Chi2 Stat': chi2_stat, 'p-value': p})\n",
    "\n",
    "# Convert results to a DataFrame for better visualization\n",
    "chi2_df = pd.DataFrame(chi2_results)\n",
    "\n",
    "# Sort features by Chi-Square statistic (descending order)\n",
    "chi2_df = chi2_df.sort_values(by='Chi2 Stat', ascending=False)\n",
    "\n",
    "print(chi2_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['Patient_Id','Patient_First_Name', 'Family_Name',\n",
    "       'Fathers_name','Institute_Name', \n",
    "       'Location_of_Institute','Test_1', 'Test_2', 'Test_3', 'Test_4','Test_5','Parental_consent'])\n",
    "#, \n",
    "  #   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Patient_Age                     1225\n",
       "Genes_Mothers_Side                 0\n",
       "Inherited_Father                 261\n",
       "Maternal_gene                   2424\n",
       "Paternal_gene                      0\n",
       "Blood_Cell_mcL                     0\n",
       "Mothers_age                     5204\n",
       "Fathers_age                     5165\n",
       "Status                             0\n",
       "Respiratory_Rate_breaths_min    1826\n",
       "Heart_Rates_Min                 1805\n",
       "Follow_up                       1862\n",
       "Gender                          1844\n",
       "Birth_asphyxia                  1848\n",
       "Autopsy_Birth_Defect            3935\n",
       "Place_of_birth                  1820\n",
       "Folic_Acid                      1814\n",
       "Maternal_Illness                1824\n",
       "Radiation_Exposure              1830\n",
       "Substance_Abuse                 1900\n",
       "Assisted_Conception             1847\n",
       "History_Previous_Pregnancies    1893\n",
       "Previous_Abortion               1828\n",
       "Birth_Defects                   1839\n",
       "White_Blood_Cell                1849\n",
       "Blood_test_result               1833\n",
       "Symptom_1                       1844\n",
       "Symptom_2                       1898\n",
       "Symptom_3                       1797\n",
       "Symptom_4                       1821\n",
       "Symptom_5                       1848\n",
       "Genetic_Disorder                   0\n",
       "Disorder_Subclass               1890\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define numeric and categorical features explicitly\n",
    "numeric_features = [\n",
    "    \"Patient_Age\", \"Blood_Cell_mcL\", \"Mothers_age\", \"Fathers_age\", \n",
    "    \"Previous_Abortion\", \"White_Blood_Cell\", \"Symptom_1\", \n",
    "    \"Symptom_2\", \"Symptom_3\", \"Symptom_4\", \"Symptom_5\"\n",
    "]\n",
    "categorical_features = [\n",
    "    \"Genes_Mothers_Side\", \"Inherited_Father\", \"Maternal_gene\", \n",
    "    \"Paternal_gene\", \"Status\", \"Respiratory_Rate_breaths_min\", \n",
    "    \"Heart_Rates_Min\", \"Follow_up\", \"Gender\", \"Birth_asphyxia\", \n",
    "    \"Autopsy_Birth_Defect\", \"Place_of_birth\", \"Folic_Acid\", \n",
    "    \"Maternal_Illness\", \"Radiation_Exposure\", \"Substance_Abuse\", \n",
    "    \"Assisted_Conception\", \"History_Previous_Pregnancies\", \n",
    "    \"Birth_Defects\", \"Blood_test_result\"\n",
    "]\n",
    "\n",
    "# Preprocessor for X\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Preprocessor for y (Label Encoding)\n",
    "label_encoder_y = LabelEncoder()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X)\n",
    "X = train_data.drop(columns=[\"Genetic_Disorder\", \"Disorder_Subclass\"])\n",
    "X_preprocessed = preprocessor_X.fit_transform(X)\n",
    "\n",
    "# Prepare target (y)\n",
    "y = train_data[\"Genetic_Disorder\"]\n",
    "y_preprocessed = label_encoder_y.fit_transform(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_preprocessed is a numpy array and column names are stored\n",
    "import numpy as np\n",
    "\n",
    "# Check for nulls in each column\n",
    "nulls_per_column = np.isnan(X_preprocessed).sum(axis=0)\n",
    "print(\"Null values per column:\\n\", nulls_per_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['Mitochondrial genetic inheritance disorders'\n",
      " 'Multifactorial genetic inheritance disorders'\n",
      " 'Single-gene inheritance diseases']\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in the target variable\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Inspect unique values in the target variable\n",
    "print(y.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mitochondrial genetic inheritance disorders'\n",
      " 'Multifactorial genetic inheritance disorders'\n",
      " 'Mitochondrial genetic inheritance disorders' ...\n",
      " 'Multifactorial genetic inheritance disorders'\n",
      " 'Mitochondrial genetic inheritance disorders'\n",
      " 'Multifactorial genetic inheritance disorders']\n"
     ]
    }
   ],
   "source": [
    "label_encoder_y.fit([\"Mitochondrial genetic inheritance disorders\", \"Multifactorial genetic inheritance disorders\", \"Single-gene inheritance diseases'\"])  # Original classes used in encoding\n",
    "\n",
    "# Reverse transform to get original labels\n",
    "y_decoded = label_encoder_y.inverse_transform(y_preprocessed)\n",
    "print(y_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y_preprocessed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Training a Logistic Regression model without class balancing or feature selection \n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Model accuracy: {score:.2f}\")\n",
    "\n",
    "# Optional: Decode predictions for interpretability\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_decoded = label_encoder_y.inverse_transform(y_pred)\n",
    "\n",
    "# Display decoded predictions\n",
    "print(\"Decoded predictions:\", y_pred_decoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection methods \n",
    " - Chi-Square test already implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regularization strength (C): 0.01\n",
      "Model accuracy after tuning: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\"C\": [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "# Grid search for optimal C\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "score = best_model.score(X_test, y_test)\n",
    "print(f\"Best regularization strength (C): {grid_search.best_params_['C']}\")\n",
    "print(f\"Model accuracy after tuning: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top important features (Lasso):\n",
      "num__Symptom_2: -0.0069\n",
      "num__Symptom_4: -0.0019\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso \n",
    "# Fit Lasso regression model\n",
    "lasso = Lasso(alpha=0.01)  # Alpha controls the regularization strength\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "feature_names = preprocessor_X.get_feature_names_out()\n",
    "# Extract important features\n",
    "coefficients = lasso.coef_\n",
    "important_features = [\n",
    "    (feature, coef) for feature, coef in zip(feature_names, coefficients) if coef != 0\n",
    "]\n",
    "\n",
    "# Sort features by importance\n",
    "important_features = sorted(important_features, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Display the top features\n",
    "print(\"Top important features (Lasso):\")\n",
    "for feature, coef in important_features[:10]:\n",
    "    print(f\"{feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top important features (Random Forest):\n",
      "num__Symptom_5: 0.0491\n",
      "num__Blood_Cell_mcL: 0.0458\n",
      "num__White_Blood_Cell: 0.0434\n",
      "num__Symptom_3: 0.0405\n",
      "num__Fathers_age: 0.0396\n",
      "num__Mothers_age: 0.0382\n",
      "num__Patient_Age: 0.0380\n",
      "num__Symptom_4: 0.0369\n",
      "num__Previous_Abortion: 0.0283\n",
      "num__Symptom_2: 0.0262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature Importance\n",
    "feature_importances = rf.feature_importances_\n",
    "important_features = sorted(\n",
    "    zip(feature_names, feature_importances),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Display top features\n",
    "print(\"Top important features (Random Forest):\")\n",
    "for feature, importance in important_features[:10]:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information\n",
    "mutual_info = mutual_info_classif(X_preprocessed, y_preprocessed, discrete_features=True)\n",
    "\n",
    "# Combine features with scores\n",
    "important_features = sorted(\n",
    "    zip(feature_names, mutual_info),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "# Display top features\n",
    "print(\"Top important features (Mutual Information):\")\n",
    "for feature, score in important_features[:10]:\n",
    "    print(f\"{feature}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implimenting SMOTE for class balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 10202 instances\n",
      "Class 1: 2071 instances\n",
      "Class 2: 7664 instances\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count target class distribution\n",
    "unique_classes, counts = np.unique(y_preprocessed, return_counts=True)\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Class {cls}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y_preprocessed)\n",
    "\n",
    "# Train-test split after resampling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 10202 instances\n",
      "Class 1: 10202 instances\n",
      "Class 2: 10202 instances\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count target class distribution\n",
    "unique_classes, counts = np.unique(y_resampled, return_counts=True)\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Class {cls}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    \"num__Blood_Cell_mcL\", \"num__White_Blood_Cell\", \"num__Symptom_5\",\n",
    "    \"num__Symptom_4\", \"num__Symptom_3\", \"num__Symptom_2\", \"num__Symptom_1\",\n",
    "    \"num__Fathers_age\", \"num__Mothers_age\", \"num__Patient_Age\",\n",
    "    \"cat__Blood_test_result_slightly abnormal\", \"cat__Birth_Defects_Multiple\",\n",
    "    \"cat__History_Previous_Pregnancies_No\"\n",
    "]\n",
    "\n",
    "X_resampled_selected = X_resampled[:, [feature_names.tolist().index(f) for f in selected_features]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split after feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_selected, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features after selection: 13\n",
      "Number of selected features: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features after selection:\", X_resampled_selected.shape[1])\n",
    "print(\"Number of selected features:\", len(selected_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (24484, 13)\n",
      "Testing set shape: (6122, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6891538712838942\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.68      0.65      2052\n",
      "           1       0.80      0.90      0.84      2024\n",
      "           2       0.63      0.50      0.55      2046\n",
      "\n",
      "    accuracy                           0.69      6122\n",
      "   macro avg       0.68      0.69      0.68      6122\n",
      "weighted avg       0.68      0.69      0.68      6122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight=\"balanced\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-All Logistic Regression Accuracy: 0.5360993139496897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train One-vs-All Logistic Regression\n",
    "ovr_model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "ovr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_ovr = ovr_model.predict(X_test)\n",
    "print(\"One-vs-All Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_ovr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Accuracy: 0.7018948056190787\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.72      0.67      2052\n",
      "           1       0.83      0.87      0.85      2024\n",
      "           2       0.63      0.52      0.57      2046\n",
      "\n",
      "    accuracy                           0.70      6122\n",
      "   macro avg       0.70      0.70      0.70      6122\n",
      "weighted avg       0.70      0.70      0.70      6122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    (\"random_forest\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"gradient_boosting\", GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Define stacking classifier\n",
    "stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_stacked = stacked_model.predict(X_test)\n",
    "print(\"Stacked Model Accuracy:\", accuracy_score(y_test, y_pred_stacked))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define diverse base models\n",
    "base_estimators = [\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('svc', SVC(probability=True, random_state=42))\n",
    "]\n",
    "\n",
    "# Stacked model\n",
    "stacked_model_diverse = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train and evaluate\n",
    "stacked_model_diverse.fit(X_train, y_train)\n",
    "y_pred_stacked_diverse = stacked_model_diverse.predict(X_test)\n",
    "print(\"Stacked Model Accuracy (XGBoost + RF + SVC):\", accuracy_score(y_test, y_pred_stacked_diverse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(stacked_model, X_balanced, y_balanced, cv=5, scoring='accuracy')\n",
    "print(\"Cross-validated accuracy (GB + XGBoost):\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implimenting  base classifiers for multi classifier apperach - All models separately "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of RandomForestClassifier: 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.65      2052\n",
      "           1       0.79      0.89      0.84      2024\n",
      "           2       0.62      0.50      0.55      2046\n",
      "\n",
      "    accuracy                           0.69      6122\n",
      "   macro avg       0.68      0.69      0.68      6122\n",
      "weighted avg       0.68      0.69      0.68      6122\n",
      "\n",
      "Accuracy of GradientBoostingClassifier: 0.64\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.78      0.67      2052\n",
      "           1       0.72      0.83      0.77      2024\n",
      "           2       0.60      0.30      0.40      2046\n",
      "\n",
      "    accuracy                           0.64      6122\n",
      "   macro avg       0.63      0.64      0.61      6122\n",
      "weighted avg       0.63      0.64      0.61      6122\n",
      "\n",
      "Accuracy of XGBClassifier: 0.66\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.71      0.65      2052\n",
      "           1       0.79      0.83      0.81      2024\n",
      "           2       0.58      0.44      0.50      2046\n",
      "\n",
      "    accuracy                           0.66      6122\n",
      "   macro avg       0.66      0.66      0.65      6122\n",
      "weighted avg       0.66      0.66      0.65      6122\n",
      "\n",
      "Accuracy of SVC: 0.61\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.70      0.62      2052\n",
      "           1       0.68      0.84      0.75      2024\n",
      "           2       0.61      0.31      0.41      2046\n",
      "\n",
      "    accuracy                           0.61      6122\n",
      "   macro avg       0.61      0.62      0.59      6122\n",
      "weighted avg       0.61      0.61      0.59      6122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "# Define base models\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "svc_model = SVC(probability=True, random_state=42)  # SVC with probability enabled\n",
    "\n",
    "# Train and evaluate each model\n",
    "models = [rf_model, gb_model, xgb_model, svc_model]\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Accuracy of {model.__class__.__name__}: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Accuracy: 0.70\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.68      2052\n",
      "           1       0.84      0.86      0.85      2024\n",
      "           2       0.63      0.51      0.56      2046\n",
      "\n",
      "    accuracy                           0.70      6122\n",
      "   macro avg       0.70      0.70      0.70      6122\n",
      "weighted avg       0.70      0.70      0.70      6122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Define base models\n",
    "base_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "    ('svc', SVC(probability=True, random_state=42))  # Enable probability for SVC\n",
    "]\n",
    "\n",
    "# Stacked model with Logistic Regression as the meta-classifier\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5  # 5-fold cross-validation for meta-model training\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_stacked = stacked_model.predict(X_test)\n",
    "\n",
    "# Evaluate the stacked model\n",
    "accuracy = accuracy_score(y_test, y_pred_stacked)\n",
    "print(f\"Stacked Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacked))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Targeted oversampling for class 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes in y_preprocessed: {0, 1, 2}\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique classes in y_preprocessed:\", set(y_preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy={0: 3000, 1: 3000, 2: 4000}, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_preprocessed shape: (19937, 61)\n",
      "y_preprocessed shape: (19937,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_preprocessed shape:\", X_preprocessed.shape)\n",
    "print(\"y_preprocessed shape:\", y_preprocessed.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 10202 samples and 3000 samples are asked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Correct SMOTE configuration\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#smote = SMOTE(sampling_strategy='auto', random_state=42)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Apply SMOTE\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Verify resampled data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResampled X shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, X_resampled\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\base.py:108\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_type\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\utils\\_validation.py:557\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28msorted\u001b[39m(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    554\u001b[0m     )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[1;32m--> 557\u001b[0m         \u001b[38;5;28msorted\u001b[39m(\u001b[43m_sampling_strategy_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    558\u001b[0m     )\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28msorted\u001b[39m(_sampling_strategy_list(sampling_strategy, y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    562\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\utils\\_validation.py:335\u001b[0m, in \u001b[0;36m_sampling_strategy_dict\u001b[1;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_sample, n_samples \u001b[38;5;129;01min\u001b[39;00m sampling_strategy\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m target_stats[class_sample]:\n\u001b[1;32m--> 335\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith over-sampling methods, the number\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of samples in a class should be greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or equal to the original number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Originally, there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_stats[class_sample]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples are asked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m             )\n\u001b[0;32m    342\u001b[0m         sampling_strategy_[class_sample] \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m-\u001b[39m target_stats[class_sample]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munder-sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 10202 samples and 3000 samples are asked."
     ]
    }
   ],
   "source": [
    "# Correct SMOTE configuration\n",
    "#smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "\n",
    "# Apply SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y_preprocessed)\n",
    "\n",
    "# Verify resampled data\n",
    "print(\"Resampled X shape:\", X_resampled.shape)\n",
    "print(\"Resampled y shape:\", y_resampled.shape)\n",
    "print(\"Class distribution after resampling:\", pd.Series(y_resampled).value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 7664 samples and 4000 samples are asked.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      2\u001b[0m smote \u001b[38;5;241m=\u001b[39m SMOTE(sampling_strategy\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m2\u001b[39m: \u001b[38;5;241m4000\u001b[39m}, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_preprocessed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\base.py:108\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[0;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_sampling_strategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sampling_type\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n\u001b[0;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    116\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\utils\\_validation.py:557\u001b[0m, in \u001b[0;36mcheck_sampling_strategy\u001b[1;34m(sampling_strategy, y, sampling_type, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28msorted\u001b[39m(SAMPLING_TARGET_KIND[sampling_strategy](y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    554\u001b[0m     )\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[1;32m--> 557\u001b[0m         \u001b[38;5;28msorted\u001b[39m(\u001b[43m_sampling_strategy_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    558\u001b[0m     )\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sampling_strategy, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict(\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28msorted\u001b[39m(_sampling_strategy_list(sampling_strategy, y, sampling_type)\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m    562\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Sarat\\miniforge3\\envs\\group11env\\Lib\\site-packages\\imblearn\\utils\\_validation.py:335\u001b[0m, in \u001b[0;36m_sampling_strategy_dict\u001b[1;34m(sampling_strategy, y, sampling_type)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_sample, n_samples \u001b[38;5;129;01min\u001b[39;00m sampling_strategy\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m target_stats[class_sample]:\n\u001b[1;32m--> 335\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith over-sampling methods, the number\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    337\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of samples in a class should be greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    338\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or equal to the original number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    339\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Originally, there is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_stats[class_sample]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    340\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples are asked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    341\u001b[0m             )\n\u001b[0;32m    342\u001b[0m         sampling_strategy_[class_sample] \u001b[38;5;241m=\u001b[39m n_samples \u001b[38;5;241m-\u001b[39m target_stats[class_sample]\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munder-sampling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: With over-sampling methods, the number of samples in a class should be greater or equal to the original number of samples. Originally, there is 7664 samples and 4000 samples are asked."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(sampling_strategy={2: 4000}, random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_preprocessed, y_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    \"num__Blood_Cell_mcL\", \"num__White_Blood_Cell\", \"num__Symptom_5\",\n",
    "    \"num__Symptom_4\", \"num__Symptom_3\", \"num__Symptom_2\", \"num__Symptom_1\",\n",
    "    \"num__Fathers_age\", \"num__Mothers_age\", \"num__Patient_Age\",\n",
    "    \"cat__Blood_test_result_slightly abnormal\", \"cat__Birth_Defects_Multiple\",\n",
    "    \"cat__History_Previous_Pregnancies_No\"\n",
    "]\n",
    "\n",
    "X_resampled_selected = X_resampled[:, [feature_names.tolist().index(f) for f in selected_features]]\n",
    "\n",
    "\n",
    "# Train-test split after feature selection\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled_selected, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Count target class distribution\n",
    "unique_classes, counts = np.unique(y_resampled, return_counts=True)\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"Class {cls}: {count} instances\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_stacked_model(X_train, y_train, X_test, y_test):\n",
    "    # Define base models\n",
    "    base_estimators = [\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
    "        ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "        ('svc', SVC(probability=True, random_state=42))  # Enable probability for SVC\n",
    "    ]\n",
    "\n",
    "    # Define the stacked model\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator=LogisticRegression(max_iter=1000),\n",
    "        cv=5  # 5-fold cross-validation for meta-model training\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the stacked model\n",
    "    stacked_model.fit(X_train, y_train)\n",
    "    y_pred_stacked = stacked_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred_stacked)\n",
    "    print(f\"Stacked Model Accuracy: {accuracy:.2f}\")\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred_stacked))\n",
    "    \n",
    "    return stacked_model, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the stacked model\n",
    "stacked_model, stacked_accuracy = evaluate_stacked_model(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_individual_models(X_train, y_train, X_test, y_test):\n",
    "    # Define base models\n",
    "    models = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "        SVC(probability=True, random_state=42)  # Enable probability for SVC\n",
    "    ]\n",
    "\n",
    "    # Train and evaluate each model\n",
    "    results = {}\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy of {model.__class__.__name__}: {accuracy:.2f}\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        results[model.__class__.__name__] = accuracy\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate individual models\n",
    "individual_results = evaluate_individual_models(X_train, y_train, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import ClassifierChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Classifier Accuracy: 0.6868670369160406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.68      0.65      2052\n",
      "           1       0.79      0.89      0.84      2024\n",
      "           2       0.62      0.50      0.55      2046\n",
      "\n",
      "    accuracy                           0.69      6122\n",
      "   macro avg       0.68      0.69      0.68      6122\n",
      "weighted avg       0.68      0.69      0.68      6122\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain Classifier using Random Forest\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "chain_classifier = ClassifierChain(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "chain_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Convert predictions to dense format\n",
    "y_pred_chain = chain_classifier.predict(X_test).toarray()\n",
    "\n",
    "# If y_test is sparse, convert it to dense format too\n",
    "if hasattr(y_test, 'toarray'):\n",
    "    y_test_dense = y_test.toarray()\n",
    "else:\n",
    "    y_test_dense = y_test\n",
    "\n",
    "# Evaluate the Chain Classifier\n",
    "print(\"Chain Classifier Accuracy:\", accuracy_score(y_test_dense, y_pred_chain))\n",
    "print(classification_report(y_test_dense, y_pred_chain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'base_estimator__n_estimators': [10, 50, 100, 200],\n",
    "    'base_estimator__max_depth': [None, 5, 10, 15],\n",
    "    'base_estimator__min_samples_split': [2, 5, 10],\n",
    "    'base_estimator__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(ClassifierChain(RandomForestClassifier(random_state=42)), param_grid, cv=5, scoring='f1_macro')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "best_chain_classifier = grid_search.best_estimator_\n",
    "y_pred_chain = best_chain_classifier.predict(X_test)\n",
    "\n",
    "print(\"Chain Classifier Accuracy:\", accuracy_score(y_test, y_pred_chain))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_chain))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "group11env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
